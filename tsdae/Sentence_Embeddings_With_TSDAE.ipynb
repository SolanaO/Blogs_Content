{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TSDAE Embeddings"
      ],
      "metadata": {
        "id": "dPzrJhtM9Hap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workspace Setup"
      ],
      "metadata": {
        "id": "xhZLy0ob9eW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install datasets\n",
        "!pip install pylatexenc"
      ],
      "metadata": {
        "id": "bu6bDNWw990b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neccessary imports\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "import random\n",
        "import time\n",
        "\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "from sentence_transformers import models, util, evaluation, losses\n",
        "from sentence_transformers import datasets\n",
        "\n",
        "import datasets as dts\n",
        "from datasets import load_dataset\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ZXtwlFq-9_O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKIW1xD6AcoX"
      },
      "outputs": [],
      "source": [
        "# Load and mount the drive helper\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the working directory\n",
        "%cd /content/drive/MyDrive/tsdae\n",
        "\n",
        "# Create a path variable\n",
        "file_path = \"/content/drive/MyDrive/tsdae\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the Pre-Training arXiv Data"
      ],
      "metadata": {
        "id": "jsC355mC-e6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use part of [Kaggle arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv), comprised of more than 1.7M scholarly STEM papers and their metadata from the established electronic preprint online platform [arXiv](https://arxiv.org). After downloading the dataset, we extract the preprints with the chosen topics. The full dataset is quite large and we have to adjust our approach to extract just parts of it.\n"
      ],
      "metadata": {
        "id": "Y7MsblccETv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the papers with subject \"math\"\n",
        "# Skip this step if you are using the math papers datafile\n",
        "\n",
        "def extract_entries_with_math(filename: str):\n",
        "    \"\"\"\n",
        "    Function to extract those entries that contain the string 'math' in the 'id'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the extracted entries.\n",
        "    entries_with_math = []\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                # Load the JSON object from the line\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # Check if the \"id\" key exists and if it contains \"math\"\n",
        "                if \"id\" in data and \"math\" in data[\"id\"]:\n",
        "                    entries_with_math.append(data)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # Print an error message if this line isn't valid JSON\n",
        "                print(f\"Couldn't parse: {line}\")\n",
        "\n",
        "    return entries_with_math\n",
        "\n",
        "# Snapshot of the arXiv dataset\n",
        "arxiv_full_dataset = file_path + \"/data/arxiv-metadata-oai-snapshot.json\"\n",
        "\n",
        "# Extract the mathematics papers\n",
        "entries = extract_entries_with_math(arxiv_full_dataset)\n",
        "\n",
        "# Save the dataset as a JSON object\n",
        "arxiv_dataset_math = file_path + \"/data/arxiv_math_dataset.json\"\n",
        "\n",
        "with open(arxiv_dataset_math, 'w') as fout:\n",
        "    json.dump(entries, fout)"
      ],
      "metadata": {
        "id": "6xUvr5CQu-nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the json file containing the selected publications\n",
        "filename = file_path+\"/data/arxiv_math_dataset.json\"\n",
        "\n",
        "with open(filename, 'r') as fin:\n",
        "        dataset = json.load(fin)"
      ],
      "metadata": {
        "id": "7jKIaJjXAk5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data as a Pandas dataframe\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# take a look at the data\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "gL8CM5zQaul2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the titles by transforming the LaTeX script\n",
        "\n",
        "parsed_titles = []\n",
        "\n",
        "for i,a in df.iterrows():\n",
        "    \"\"\"\n",
        "    Function to replace LaTeX script with ISO code.\n",
        "    \"\"\"\n",
        "    # Parse titles\n",
        "    try:\n",
        "        parsed_titles.append(LatexNodes2Text().latex_to_text(a['title']).replace('\\n', ' ').strip())\n",
        "    except:\n",
        "        parsed_titles.append(a['title'].replace('\\n', ' ').strip())\n",
        "\n",
        "df['parsed_title'] = parsed_titles"
      ],
      "metadata": {
        "id": "04p15Br1ZhkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the titles as a list\n",
        "train_sentences = df.parsed_title.to_list()\n",
        "# The size of the dataset\n",
        "len(train_sentences)"
      ],
      "metadata": {
        "id": "_EKmoeZPBREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the special denoising dataset that adds noise to the data\n",
        "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)"
      ],
      "metadata": {
        "id": "S5F3_j_mCCJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an example\n",
        "print(train_dataset[2010])"
      ],
      "metadata": {
        "id": "wUuIHmzUa1pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch data loader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "FlshXxmeDFIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model & Pre-Training"
      ],
      "metadata": {
        "id": "kq1hEQsjmmAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a model to build the word embeddings\n",
        "model_name = 'bert-base-uncased'\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "\n",
        "# Choose the pooling method for the word embeddings\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
        "\n",
        "# Build the sentence transformer using the two modules\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "metadata": {
        "id": "fvvpKNJeCkts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the denoising auto-encoder loss and tie encoder-decoder weights\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(model,\n",
        "                                             decoder_name_or_path=model_name,\n",
        "                                             tie_encoder_decoder=True)"
      ],
      "metadata": {
        "id": "niCYXEIkDKE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the clock\n",
        "start_time = time.time()\n",
        "\n",
        "# Call the fit method to train the model\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=1,\n",
        "    weight_decay=0,\n",
        "    scheduler='constantlr',\n",
        "    optimizer_params={'lr': 3e-5},\n",
        "    show_progress_bar=True,\n",
        "    use_amp=True # set to False if GPU does not support FP16 cores\n",
        ")\n",
        "\n",
        "# Stop the clock\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"Model trained in {elapsed_time:.2f} seconds, on a Google Colab Pro with A100 GPU & High-RAM.\")\n",
        "\n",
        "# Save path of the model\n",
        "pretrained_model_save_path = 'output/tsdae-bert-uncased-math'\n",
        "# Save the model locally\n",
        "model.save(pretrained_model_save_path)"
      ],
      "metadata": {
        "id": "CJ1kzAC7DLXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate TSDAE Pre-Trained Model"
      ],
      "metadata": {
        "id": "hhf4UjRdphIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform standard evaluation on benchmark STS datasets. These evaluations however are not using the specific domain we are working with and might not reflect the true performance of the model."
      ],
      "metadata": {
        "id": "KuEcp5QJrHER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save path of the model\n",
        "pre_model_save_path = 'output/tsdae-bert-uncased-math'\n",
        "# Load the model if necessary\n",
        "model = SentenceTransformer(pre_model_save_path)"
      ],
      "metadata": {
        "id": "gVQqOBBfAgOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the STS benchmark dataset from HuggingFace\n",
        "sts = dts.load_dataset('glue', 'stsb', split='validation')\n",
        "\n",
        "# Take a peek at the dataset\n",
        "sts"
      ],
      "metadata": {
        "id": "YfBJt4lH07oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a peek at one of the entries\n",
        "sts['idx'][100], sts['sentence1'][100], sts['sentence2'][100], sts['label'][100]"
      ],
      "metadata": {
        "id": "vhcf5B-O6_Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the [0, 5] range to [0, 1]\n",
        "sts = sts.map(lambda x: {'label': x['label'] / 5.0})\n",
        "\n",
        "# Create a list to store the parsed data\n",
        "samples = []\n",
        "\n",
        "for sample in sts:\n",
        "    # Reformat to use InputExample\n",
        "    samples.append(InputExample(\n",
        "        texts=[sample['sentence1'], sample['sentence2']],\n",
        "        label=sample['label']\n",
        "    ))\n",
        "\n",
        "# Instantiate the evaluation module\n",
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    samples, write_csv=False\n",
        ")"
      ],
      "metadata": {
        "id": "tSMa7KkJ6qF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score estimation of the pre-trained model\n",
        "ev_tsdae = evaluator(model)\n",
        "print(f\"The score for the TSDAE pre-trained model is: {ev_tsdae}.\")"
      ],
      "metadata": {
        "id": "-xhEZAqq2pV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score estimation for the baseline model with CLS pooling\n",
        "bert = models.Transformer('bert-base-uncased')\n",
        "pooling = models.Pooling(bert.get_word_embedding_dimension(), 'cls')\n",
        "\n",
        "bert_model = SentenceTransformer(modules=[bert, pooling])\n",
        "ev_bert_base_uncased = evaluator(bert_model)\n",
        "print(f\"The score for the baseline model is: {ev_bert_base_uncased}.\")"
      ],
      "metadata": {
        "id": "laPG3DF_25Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score estimation on a full scope advanced pretrained model\n",
        "all_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "ev_all_mpnet_base = evaluator(all_model)\n",
        "print(f\"The score for the all-mpnet-base-v2 is: {ev_all_mpnet_base}.\")"
      ],
      "metadata": {
        "id": "_58m15953Uf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score estimation on a second full scope more advanced pretrained model\n",
        "# This model was tuned for semantic search\n",
        "qa_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
        "ev_multi_qa_mpnet_base = evaluator(qa_model)\n",
        "print(f\"The score for the multi-qa-mpnet-base-dot-v1 model is: {ev_multi_qa_mpnet_base}.\")"
      ],
      "metadata": {
        "id": "b-FzpSKv4bQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning on AllNLI Dataset"
      ],
      "metadata": {
        "id": "vszhDl8irF-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Prepare the Datasets"
      ],
      "metadata": {
        "id": "poAGayoauCgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is based on [sentence-transformers/examples/training/nli/training_nli_v2.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py)."
      ],
      "metadata": {
        "id": "y7X2gfm5vZmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if dataset exist. If not, download and extract  it\n",
        "nli_dataset_path = 'data/AllNLI.tsv.gz'\n",
        "sts_dataset_path = 'data/stsbenchmark.tsv.gz'\n",
        "\n",
        "if not os.path.exists(nli_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
        "\n",
        "if not os.path.exists(sts_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)"
      ],
      "metadata": {
        "id": "NZFFosSOvFXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the dataset\n",
        "def add_to_samples(sent1, sent2, label):\n",
        "    if sent1 not in train_data:\n",
        "        train_data[sent1] = {'contradiction': set(), 'entailment': set(), 'neutral': set()}\n",
        "    train_data[sent1][label].add(sent2)\n",
        "\n",
        "train_data = {}\n",
        "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        if row['split'] == 'train':\n",
        "            sent1 = row['sentence1'].strip()\n",
        "            sent2 = row['sentence2'].strip()\n",
        "\n",
        "            add_to_samples(sent1, sent2, row['label'])\n",
        "            add_to_samples(sent2, sent1, row['label'])  # Also add the opposite\n",
        "\n",
        "train_samples = []\n",
        "for sent1, others in train_data.items():\n",
        "    if len(others['entailment']) > 0 and len(others['contradiction']) > 0:\n",
        "        train_samples.append(InputExample(texts=[sent1, random.choice(list(others['entailment'])), random.choice(list(others['contradiction']))]))\n",
        "        train_samples.append(InputExample(texts=[random.choice(list(others['entailment'])), sent1, random.choice(list(others['contradiction']))]))\n"
      ],
      "metadata": {
        "id": "Z5QxgapCumYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the size of the finetuning dataset\n",
        "len(train_samples)"
      ],
      "metadata": {
        "id": "epn_AhOXvwQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special data loader that avoid duplicates within a batch\n",
        "train_dataloader = datasets.NoDuplicatesDataLoader(train_samples,\n",
        "                                                   batch_size=32) # 128 default, use smaller batch to fit on Colab"
      ],
      "metadata": {
        "id": "VoeN9GGT72LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tune the TSDAE Pretrained Model"
      ],
      "metadata": {
        "id": "7r6Hr6WZw0HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system trains BERT (or any other transformer model like RoBERTa, DistilBERT etc.) on the SNLI + MultiNLI (AllNLI) dataset\n",
        "with MultipleNegativesRankingLoss. Entailments are positive pairs and the contradiction on AllNLI dataset is added as a hard negative.\n",
        "At every 10% training steps, the model is evaluated on the STS benchmark dataset"
      ],
      "metadata": {
        "id": "E9HYrfNYwE5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model parameters\n",
        "model_name = 'output/tsdae-bert-uncased-math'\n",
        "train_batch_size = 32 # The larger, the better the results (usually), but requires more GPU memory\n",
        "max_seq_length = 75\n",
        "num_epochs = 1\n"
      ],
      "metadata": {
        "id": "Rdlh_c709C0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the local model\n",
        "local_model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "7HyTTWPixK6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our training loss\n",
        "train_loss = losses.MultipleNegativesRankingLoss(local_model)"
      ],
      "metadata": {
        "id": "Zxkn1mUh0hH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read STS benchmark dataset and use it as evaluation set\n",
        "dev_samples = []\n",
        "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        if row['split'] == 'dev':\n",
        "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
        "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
        "\n",
        "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples,\n",
        "                                                                 batch_size=train_batch_size,\n",
        "                                                                 name='sts-dev')\n"
      ],
      "metadata": {
        "id": "TyQoAGUGs1ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the training\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) # 10% of train data for warm-up\n",
        "\n",
        "# Path to save the finetuned model\n",
        "model_save_path = 'output/finetuned-bert-uncased-math'\n",
        "\n",
        "# Start the clock\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "local_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=dev_evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=int(len(train_dataloader)*0.1),\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path,\n",
        "          use_amp=True          # Set to True, if your GPU supports FP16 operations\n",
        "          )\n",
        "\n",
        "# Stop the clock\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"Model finetuned in {elapsed_time:.2f} seconds, on a Google Colab Pro with A100 GPU & High-RAM.\")\n"
      ],
      "metadata": {
        "id": "NqIRsj-wxwZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score estimation of the finetuned model\n",
        "ev_finetuned = evaluator(local_model)\n",
        "print(f\"The score for the finetuned model is: {ev_finetuned}.\")"
      ],
      "metadata": {
        "id": "1m89UN4FGIxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuL0HNP4LxL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}