{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUkJckjq8OI_"
      },
      "source": [
        "## Workspace Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Neccessary Installs\n",
        "\n",
        "!pip install -q groq\n",
        "\n",
        "#!pip install torch\n",
        "!pip install -U accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U datasets\n",
        "!pip install -U evaluate\n",
        "!pip install -U ninja\n",
        "!pip install -U packaging\n",
        "!pip install -U peft\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U transformers\n",
        "!pip install -U trl"
      ],
      "metadata": {
        "id": "dBAxJUdZPHSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Google Colab Drive Helper\n",
        "\n",
        "# For Google Colab settings\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory\n",
        "%cd '/content/drive/MyDrive/postedBlogs/llama3RE'"
      ],
      "metadata": {
        "id": "m9UEQ-hmhTV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hugging Face Credentials\n",
        "\n",
        "# For Hugging Face Hub setting\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Upload the HuggingFace token (should have WRITE access) from Colab secrets\n",
        "HF = userdata.get('HF')\n",
        "\n",
        "# This is needed to upload the model to HuggingFace\n",
        "login(token=HF,add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "sjS6W3l6iTH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path Variables\n",
        "\n",
        "# Create a path variable for the data folder\n",
        "data_path = '/content/drive/MyDrive/postedBlogs/llama3RE/datas/'\n",
        "\n",
        "# SFT dataset contains extracted sentences and gold_re\n",
        "sft_data_path = f'{data_path}sft_dataset.json'\n",
        "\n",
        "# Data collected from the the mini-test\n",
        "mini_data_path = f'{data_path}mini_data.json'\n",
        "\n",
        "# Test data containing all three outputs\n",
        "all_tests_data = f'{data_path}all_tests.json'\n",
        "\n",
        "# The adjusted training dataset\n",
        "train_data_path = f'{data_path}sft_train_data.json'\n",
        "\n",
        "# Create a path variable for the SFT model to be saved locally\n",
        "sft_model_path = '/content/drive/MyDrive/postedBlogs/llama3RE/Llama3_RE/'"
      ],
      "metadata": {
        "id": "EpGhLtAi9a6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMR6rDAh3J3n"
      },
      "source": [
        "# Relation Extraction Synthetic Dataset with Llama3-70B"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Prepare Dataset"
      ],
      "metadata": {
        "id": "ctgTF9iuGXrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Dolly-15k Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
        "\n",
        "# Display an instance\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "TYtY2ZaGhRx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Determine Available Categories in Dataset\n",
        "\n",
        "dataset_categories = set([e[\"category\"] for e in dataset[\"train\"]])\n",
        "dataset_categories"
      ],
      "metadata": {
        "id": "staMuvt1jBWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse Data\n",
        "\n",
        "# Choose the desired category from the dataset\n",
        "ie_category = [e for e in dataset[\"train\"] if e[\"category\"]==\"information_extraction\"]\n",
        "\n",
        "# Retain only the context from each instance\n",
        "ie_context = [e[\"context\"] for e in ie_category]\n",
        "\n",
        "# Split the text into sentences (at the period) and keep the first sentence\n",
        "reduced_context = [text.split('.')[0] + '.' for text in ie_context]\n",
        "\n",
        "# Retain sequences of specified lengths only (use character length)\n",
        "sampler = [e for e in reduced_context if 30 < len(e) < 170]\n",
        "\n",
        "print(f\"There are {len(sampler)} instances in the dataset.\\n\")\n",
        "\n",
        "# Display several samples from the selected dataset\n",
        "sampler[110:120]"
      ],
      "metadata": {
        "id": "NFt6DepMjU0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Synthetic RE Dataset"
      ],
      "metadata": {
        "id": "nGVFtKe7Hwf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create a System Message\n",
        "\n",
        "system_message = \"\"\"You are an experienced annontator. Extract all entities and the relations between them from the following text. Write the answer as a triple entity1|relationship|entitity2. Do not add anything else.\n",
        "Example Text: Alice is from France.\n",
        "Answer: Alice|is from|France.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tzfzFwWpouTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build the Messages List\n",
        "messages = [[\n",
        "    {\"role\": \"system\",\"content\": f\"{system_message}\"},\n",
        "    {\"role\": \"user\", \"content\": e}] for e in sampler]\n",
        "messages[10]"
      ],
      "metadata": {
        "id": "67-7U_n6u3zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instantiate Groq Client\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "gclient = Groq(\n",
        "    api_key=userdata.get(\"GROQ\"),\n",
        ")"
      ],
      "metadata": {
        "id": "9-kNF5UdM1x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_data(prompt):\n",
        "\n",
        "    \"\"\"Send one request and retrieve model's generation.\"\"\"\n",
        "\n",
        "    chat_completion = gclient.chat.completions.create(\n",
        "        messages=prompt, # input prompt to send to the model\n",
        "        model=\"llama3-70b-8192\", # according to GroqCloud labeling\n",
        "        temperature=0.5, # controls diversity\n",
        "        max_tokens=128, # max number tokens to generate\n",
        "        top_p=1, # proportion of likelihood weighted options to consider\n",
        "        stop=None, # string that signals to stop generating\n",
        "        stream=False, # if set partial messages are sent\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "\n",
        "def send_messages(messages):\n",
        "\n",
        "    \"\"\"Process messages in batches with a pause between batches.\"\"\"\n",
        "\n",
        "    answers=[]\n",
        "    batch_size=10\n",
        "\n",
        "    for i in tqdm(range(0, len(messages), batch_size)):\n",
        "\n",
        "        batch = messages[i:i+10]  # get the next batch of messages\n",
        "\n",
        "        for message in batch:\n",
        "            output = process_data(message)\n",
        "            answers.append(output)\n",
        "\n",
        "        if i + 10 < len(messages):  # check if there are batches left\n",
        "            time.sleep(10)  # wait for 10 seconds\n",
        "\n",
        "    return answers"
      ],
      "metadata": {
        "id": "4V_I-esHalWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate the Data\n",
        "\n",
        "answers = send_messages(messages)\n",
        "len(answers)"
      ],
      "metadata": {
        "id": "3w_z7emP2C6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine Data with Generated Dataset\n",
        "combined_dataset = [{'text': user, 'gold_re': output} for user, output in zip(sampler, answers)]\n",
        "\n",
        "# Print the combined list to check\n",
        "combined_dataset[22]"
      ],
      "metadata": {
        "id": "9lZaw7iztDax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save the Combined Dataset\n",
        "\n",
        "import json\n",
        "\n",
        "with open(sft_data_path, 'w') as file:\n",
        "    json.dump(combined_dataset, file)"
      ],
      "metadata": {
        "id": "RM5hwi9-tfBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Llama3-8B on Relation Extraction Task"
      ],
      "metadata": {
        "id": "iIhgi6krQf8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build a Samples Dataset\n",
        "\n",
        "import random\n",
        "random.seed(17)\n",
        "\n",
        "# Select 20 random entries\n",
        "mini_data = random.sample(combined_dataset, 20)\n",
        "\n",
        "# Build conversational format\n",
        "parsed_mini_data = [[{'role': 'system', 'content': system_message},\n",
        "                     {'role': 'user', 'content': e['text']}] for e in mini_data]\n",
        "\n",
        "parsed_mini_data[1]"
      ],
      "metadata": {
        "id": "zF_ubOegcZnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create a Training Set for FineTuning\n",
        "\n",
        "train_data = [item for item in combined_dataset if item not in mini_data]\n",
        "len(train_data)"
      ],
      "metadata": {
        "id": "bO-BOul4jdQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Function\n",
        "\n",
        "def process_data(prompt):\n",
        "\n",
        "    \"\"\"Send one request and retrieve model's generation.\"\"\"\n",
        "\n",
        "    chat_completion = gclient.chat.completions.create(\n",
        "        messages=prompt, # input prompt to send to the model\n",
        "        model=\"llama3-8b-8192\", # according to GroqCloud labeling\n",
        "        temperature=0.5, # controls diversity\n",
        "        max_tokens=128, # max number tokens to generate\n",
        "        top_p=1, # proportion of likelihood weighted options to consider\n",
        "        stop=None, # string that signals to stop generating\n",
        "        stream=False, # if set partial messages are sent\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "940fONzPbvg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform RE on Samples Data with Llama-8B\n",
        "\n",
        "outputs = []\n",
        "for message in parsed_mini_data:\n",
        "    output = process_data(message)\n",
        "    outputs.append(output)\n",
        "\n",
        "outputs[3]"
      ],
      "metadata": {
        "id": "HJwsY0KBdRdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine the Samples Data with Generated RE Data\n",
        "\n",
        "# Adding new key 'test_re' with values from the list\n",
        "for i, dct in enumerate(mini_data):\n",
        "    dct['test_re'] = outputs[i]\n",
        "\n",
        "mini_data[2]"
      ],
      "metadata": {
        "id": "IouW6aygbty3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Llama3 70B and 8B RE Outputs on Samples\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Create a dataframe from collected data\n",
        "df = pd.DataFrame(mini_data)\n",
        "df"
      ],
      "metadata": {
        "id": "68yc-vg7gMEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save the Datasets\n",
        "\n",
        "import json\n",
        "\n",
        "# Data collected from the mini-test\n",
        "with open(mini_data_path, 'w') as file:\n",
        "    json.dump(mini_data, file)\n",
        "\n",
        "# The adjusted training dataset\n",
        "with open(train_data_path, 'w') as file:\n",
        "    json.dump(train_data, file)"
      ],
      "metadata": {
        "id": "MHwLD5ukj6T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Fine-Tuning Llama3-8B"
      ],
      "metadata": {
        "id": "4niI_IHyk88V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Libraries Versions\n",
        "\n",
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "import trl\n",
        "\n",
        "print(f\"The PyTorch version is {torch.__version__}.\")\n",
        "print(f\"Datasets version is {datasets.__version__}.\")\n",
        "print(f\"Transformers version is {transformers.__version__}.\")\n",
        "print(f\"TRL version is {trl.__version__}.\")"
      ],
      "metadata": {
        "id": "SJCLzh5kl884"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Assert Cuda Capabilities for Flash Attention\n",
        "\n",
        "# Assert Cuda Capability for Flash Attention\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Cuda major version: {major_version}.\\nCuda minor version: {minor_version}\")\n",
        "\n",
        "# adapted from: https://github.com/mlabonne/llm-course\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    # Limit the number of jobs to accomodate the compute capabilities\n",
        "    %env MAX_JOBS=2 # for Google Colab\n",
        "\n",
        "    # Install flash attention - for Ampere GPUs\n",
        "    %pip install flash-attn -q --no-build-isolation\n",
        "\n",
        "    torch_dtype = torch.bfloat16\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "\n",
        "else:\n",
        "    torch_dtype = torch.float16\n",
        "    attn_implementation = \"eager\"\n",
        "\n",
        "print(f\"torch_dtype = {torch_dtype}\")\n",
        "print(f\"attn_implementation = {attn_implementation}\")"
      ],
      "metadata": {
        "id": "6njW4Nnd3IlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Resources Estimation\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "ue9ckYIfmKr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LLM Model Name\n",
        "\n",
        "model_id  =  \"meta-llama/Meta-Llama-3-8B\""
      ],
      "metadata": {
        "id": "ZIQjxJHNnYvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4Lgk3EV4d"
      },
      "source": [
        "## Prepare the SFT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the SFT Dataset\n",
        "import json\n",
        "\n",
        "with open(train_data_path, 'rb') as f:\n",
        "\ttrain_data = json.load(f)\n",
        "\n",
        "train_data[123]"
      ],
      "metadata": {
        "id": "0a-BpuiJn0Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to Parse to Conversational Format\n",
        "\n",
        "# Create the System Message\n",
        "\n",
        "system_message = \"\"\"You are an experienced annontator. Extract all entities and the relations between them from the following text. Write the answer as a triple entity1|relationship|entitity2. Do not add anything else.\n",
        "Example Text: Alice is from France.\n",
        "Answer: Alice|is from|France.\n",
        "\"\"\"\n",
        "\n",
        "def create_conversation(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": sample[\"text\"]},\n",
        "            {\"role\": \"assistant\", \"content\": sample[\"gold_re\"]}\n",
        "        ]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Pqavxi0I0Cxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert Data to HuggingFace Format\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "\n",
        "# Transform to conversational format\n",
        "train_dataset = train_dataset.map(create_conversation,\n",
        "                      remove_columns=train_dataset.features,\n",
        "                      batched=False)\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "By4hTZimrgpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display a Sample\n",
        "train_dataset[\"messages\"][123]"
      ],
      "metadata": {
        "id": "UoydSzvA06W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zipOr9nvg901"
      },
      "source": [
        "## Tokenizer and Chat Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56cKo54whJCk"
      },
      "outputs": [],
      "source": [
        "#@title Load the Tokenizer\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
        "                                          use_fast=True,\n",
        "                                          trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Set a maximum length\n",
        "tokenizer.model_max_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tj__2CniFm5"
      },
      "outputs": [],
      "source": [
        "#@title Quantization Parameters\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Device Map\n",
        "\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None"
      ],
      "metadata": {
        "id": "YyoRd_vn3lA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5cIPbmYV14L"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from trl import setup_chat_format\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=device_map,\n",
        "    attn_implementation=attn_implementation,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LoRA Configuration\n",
        "\n",
        "from peft import LoraConfig\n",
        "\n",
        "# According to Sebastian Raschka findings\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=128, #32\n",
        "        lora_dropout=0.05,\n",
        "        r=256,  #16\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"o_proj\", \"gate_proj\", \"up_proj\",\n",
        "                        \"down_proj\", \"k_proj\", \"v_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "rn3bzMSz4PI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Arguments\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Adapted from  Phil Schmid blogpost\n",
        "args = TrainingArguments(\n",
        "    output_dir=sft_model_path,              # directory to save the model and repository id\n",
        "    num_train_epochs=2,                     # number of training epochs\n",
        "    per_device_train_batch_size=4,          # batch size per device during training\n",
        "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory, use in distributed training\n",
        "    #gradient_checkpointing_kwargs={\"use_reentrant\": False}, # for more stability in distributed training, it can use more memory\n",
        "    optim=\"adamw_8bit\",                     # choose paged_adamw_8bit if noy enough memory\n",
        "    logging_steps=10,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    push_to_hub=True,                      # push model to Hugging Face hub\n",
        "    hub_model_id=\"llama3-8b-sft-qlora-re\",\n",
        "    report_to=\"tensorboard\",               # report metrics to tensorboard\n",
        ")"
      ],
      "metadata": {
        "id": "W5YJsQBs4r2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMO30PnpHsAa"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the SFTTrainer\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False, # True if the dataset is large\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # the template adds the special tokens\n",
        "        \"append_concat_token\": False, # no need to add additional separator token\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImceNWv-jTfJ"
      },
      "outputs": [],
      "source": [
        "#@title Train tand Save the Model\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2jj52sBjis7"
      },
      "outputs": [],
      "source": [
        "#@title Save Model Locally\n",
        "\n",
        "#trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6tubmSYi23o"
      },
      "outputs": [],
      "source": [
        "#@title Clear Memory\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "del model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with SFT Model"
      ],
      "metadata": {
        "id": "WwvWbQO8czKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Peft Model\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# HF model\n",
        "peft_model_id = \"solanaO/llama3-8b-sft-qlora-re\"\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  offload_buffers=True\n",
        ")"
      ],
      "metadata": {
        "id": "7s65xPFxcyTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id =  tokenizer.eos_token_id\n",
        "tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "ODvcs3FrdGAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text Generation Pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "KInMbBY1ehVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the Samples Dataset\n",
        "import json\n",
        "\n",
        "with open(mini_data_path, 'rb') as f:\n",
        "\tmini_data = json.load(f)\n",
        "\n",
        "mini_data[12]"
      ],
      "metadata": {
        "id": "R6ekoiniJJ1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to Parse to Conversational Format\n",
        "\n",
        "# Create the System Message\n",
        "\n",
        "system_message = \"\"\"You are an experienced annontator. Extract all entities and the relations between them from the following text. Write the answer as a triple entity1|relationship|entitity2. Do not add anything else.\n",
        "Example Text: Alice is from France.\n",
        "Answer: Alice|is from|France.\n",
        "\"\"\"\n",
        "\n",
        "def create_input_prompt(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": sample[\"text\"]},\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "tQmrU3a1SZis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert Data to HuggingFace Format\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "test_dataset = Dataset.from_list(mini_data)\n",
        "\n",
        "# Transform to conversational format\n",
        "test_dataset = test_dataset.map(create_input_prompt,\n",
        "                      remove_columns=test_dataset.features,\n",
        "                      batched=False)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "M0pZPdSQJYqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Sample Test"
      ],
      "metadata": {
        "id": "Sv-ZJLqyUywG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate the Input Prompt\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(test_dataset[10][\"messages\"][:2],\n",
        "                                            tokenize=False,\n",
        "                                            add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "opWn1TeMdwNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate the Output\n",
        "\n",
        "outputs = pipe(prompt,\n",
        "              max_new_tokens=128,\n",
        "              do_sample=True,\n",
        "              temperature=0.01,\n",
        "              top_k=50,\n",
        "              top_p=0.1,\n",
        "              )"
      ],
      "metadata": {
        "id": "xspKp_YBdz9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Sample Outputs\n",
        "\n",
        "print(f\"Question: {mini_data[10]['text']}\\n\")\n",
        "print(f\"Gold-RE: {mini_data[10]['gold_re']}\\n\")\n",
        "print(f\"LLama3-8B-RE: {mini_data[10]['test_re']}\\n\")\n",
        "print(f\"SFT-Llama3-8B-RE: {outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "id": "dLYo3IWed-2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test on All 20 Samples\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2],\n",
        "                                                tokenize=False,\n",
        "                                                add_generation_prompt=True)\n",
        "    outputs = pipe(prompt,\n",
        "                   max_new_tokens=128,\n",
        "                   do_sample=True,\n",
        "                   temperature=0.7,\n",
        "                   top_k=50,\n",
        "                   top_p=0.95\n",
        "                   )\n",
        "\n",
        "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    return predicted_answer\n",
        "\n",
        "\n",
        "# Iterate over test dataset and predict\n",
        "sft_generation = []\n",
        "for s in tqdm(test_dataset, desc=\"Processing dataset\"):\n",
        "    sft_generation.append(evaluate(s))"
      ],
      "metadata": {
        "id": "NBOUzi8deFn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine All Test Data and Save\n",
        "import json\n",
        "\n",
        "for d, s in zip(mini_data, sft_generation):\n",
        "    d['sft_re'] = s\n",
        "\n",
        "# Data collected from the mini-test\n",
        "with open(all_tests_data, 'w') as file:\n",
        "    json.dump(mini_data, file)"
      ],
      "metadata": {
        "id": "2jr3kYPIWZo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display the Tests Results\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.DataFrame(mini_data)\n",
        "df"
      ],
      "metadata": {
        "id": "nZK9Q8lZXpSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwUmZo6sXhXh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}