{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KeyBERT -> KeyLLM -> HDBSCAN-> Neo4j"
      ],
      "metadata": {
        "id": "OSq2V0m-RrdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Setup\n",
        "\n",
        "Set the Google Colab to access files on the drive."
      ],
      "metadata": {
        "id": "1V8TaWgVVDS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and mount the drive helper\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory\n",
        "%cd '/content/drive/MyDrive/keyLLM/'"
      ],
      "metadata": {
        "id": "Ic__w8jpUyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a path variable for the data folder\n",
        "data_path = '/content/drive/MyDrive/keyLLM/datas/'\n",
        "\n",
        "# Created by the data parser, selected articles\n",
        "topic = 'cs'\n",
        "extracted_data_file = f'selected_{topic}.csv'\n",
        "\n",
        "# Data and the extracted keywords\n",
        "extracted_keys_file = 'selected_cs_keys.csv'\n",
        "\n",
        "# Articles with 5 keywords/keyphrases\n",
        "parsed_keys_file = 'parsed_cs_keys.csv'\n",
        "\n",
        "# Pickled data containing 768-dim and 10-dim embeddings, and clusters\n",
        "embeddings_clusters_keys_file = 'embedddings_clusters_cs_keys.pkl'\n",
        "\n",
        "# Descriptions and labels for keywords clusters\n",
        "descriptions_labels_keys_file = 'descriptions_labels.csv'"
      ],
      "metadata": {
        "id": "IsOJukqXa5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Load & Preparation\n"
      ],
      "metadata": {
        "id": "kU7XnCqtU6i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data from [Kaggle ArXiv Site](https://www.kaggle.com/datasets/Cornell-University/arxiv) to the specified data_path. This will download a single file `archive.zip` (1.2 GB). After unzipping a single file `archive-metadata-oai-snapshot.json` (3.7 GB) is extracted."
      ],
      "metadata": {
        "id": "Nzej78h6a-NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data parser module\n",
        "from utils.arxiv_parser import *\n",
        "\n",
        "# Initialize the data parser\n",
        "parser = ArXivDataProcessor(data_path)"
      ],
      "metadata": {
        "id": "9EqRszHSptnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file and extract a json file in data_path\n",
        "parser.unzip_file()"
      ],
      "metadata": {
        "id": "D7ctGVzrZfwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a topic and extract the articles on that topic\n",
        "entries = parser.select_topic(\"cs\")\n",
        "# Check the output for one article\n",
        "entries[111]"
      ],
      "metadata": {
        "id": "KaBLNZD6Yz2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comments**\n",
        "\n",
        "We'll chose only articles that have abstracts with token length in our interval of choice. We also have the option to retain the features that are relevant for the project and discard the remaining ones. If no columns are specified all the features will be retained.\n",
        "\n",
        "To keep track of the number of tokens in the abstract, a new column `abs_length` is added. There is an option to keep that column, in case we want to gain further insight into the length distribution of the abstracts.\n",
        "\n",
        "There is also an option to build a corpus which corresponds to the concatenation of the title and the abstract."
      ],
      "metadata": {
        "id": "uhP2mF9VkeCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain only the selected data\n",
        "df = parser.select_articles(entries, # extracted articles\n",
        "                            cols=['id', 'title', 'abstract'], # features to keep\n",
        "                            min_length = 100, # min tokens an abstract should have\n",
        "                            max_length = 120, # max tokens an abstract should have\n",
        "                            keep_abs_length = False, # do not keep the abs_length column\n",
        "                            build_corpus=False) # do not build a corpus column\n",
        "# Inspect the data\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "lrzXCLIwjRlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the selected data to a csv file 'selected_{topic}.csv', uses data_path\n",
        "parser.save_selected_data(df, 'cs')"
      ],
      "metadata": {
        "id": "P-TmKNxckXr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KeyBERT & LLM to Extract Keywords"
      ],
      "metadata": {
        "id": "XLjzw-hKlUAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Installs KeyBERT & LLM & SentenceTransformers"
      ],
      "metadata": {
        "id": "GlnhZBAUdCe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages\n",
        "\n",
        "!pip install transformers optimum accelerate\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install keybert\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "Xd18Ka-rrL_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the LLM and the Tokenizer"
      ],
      "metadata": {
        "id": "bltUlsc5VJps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "\n",
        "# To use a different branch, change revision\n",
        "llm = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "pmo3zNzoUt1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the text generation pipeline\n",
        "generator = pipeline(\n",
        "    model=llm,\n",
        "    tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    max_new_tokens=50,\n",
        "    repetition_penalty=1.1,\n",
        ")"
      ],
      "metadata": {
        "id": "S5helstyXZII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate KeyBERT"
      ],
      "metadata": {
        "id": "y6RjL-NtVRar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt is very important for the quality of the text generation. Experimented with variations such as using technical terms from computer science or asking to remove the stopwords. Both these requests make the model to add all sort of comments and opinions! This format generates the cleanest output I was able to get."
      ],
      "metadata": {
        "id": "UkycGG6cq25H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom prompt, use the format from the HF model card\n",
        "# https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
        "\n",
        "prompt_keywords= \"\"\"\n",
        "<|system|>\n",
        "I have the following document:\n",
        "Semantics and Termination of Simply-Moded Logic Programs with Dynamic Scheduling\n",
        "and five candidate keywords:\n",
        "scheduling, logic, semantics, termination, moded\n",
        "\n",
        "Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\n",
        "Follow the requirements below:\n",
        "1. Make sure to extract only the keywords or keyphrases that appear in the text.\n",
        "2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n",
        "3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!\n",
        "\n",
        "semantics, termination, simply-moded, logic programs, dynamic scheduling</s>\n",
        "\n",
        "<|user|>\n",
        "I have the following document:\n",
        "[DOCUMENT]\n",
        "and five candidate keywords:\n",
        "[CANDIDATES]\n",
        "\n",
        "Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\n",
        "Follow the requirements below:\n",
        "1. Make sure to extract only the keywords or keyphrases that appear in the text.\n",
        "2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n",
        "3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!</s>\n",
        "\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PcWukpFp23vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert.llm import TextGeneration\n",
        "from keybert import KeyLLM, KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Instantiate KeyBert TextGeneration pipeline wrapper\n",
        "# source: https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py\n",
        "\n",
        "llm_tg = TextGeneration(generator, prompt=prompt_keywords)\n",
        "\n",
        "# Specify the embedding model: cluster similar docs and extracts keywords from community center document\n",
        "# Based on: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py\n",
        "\n",
        "kw_model= KeyBERT(llm=llm_tg, model = \"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "wXx_tXnawsxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Keywords with KeyBERT & LLM"
      ],
      "metadata": {
        "id": "2bmkwf5SWDLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process documents in batches\n",
        "\n",
        "def process_in_batches(corpus,\n",
        "                       batch_size):\n",
        "    results = []\n",
        "    for i in range(0, len(corpus), batch_size):\n",
        "        batch = corpus[i:i+batch_size]\n",
        "        keys = kw_model.extract_keywords(batch,\n",
        "                                         threshold=0.5)\n",
        "        results.extend(keys)\n",
        "    return results"
      ],
      "metadata": {
        "id": "rUV0COYt53uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset if necessary\n",
        "df = pd.read_csv(data_path+extracted_data_file)\n",
        "\n",
        "# Retain the articles titles only for analysis\n",
        "titles_list = df.title.tolist()\n",
        "\n",
        "# Process the documents and collect the results\n",
        "titles_keys = process_in_batches(titles_list,\n",
        "                                 batch_size = 100)\n",
        "\n",
        "# Add the results to df\n",
        "df[\"titles_keys\"] = titles_keys\n",
        "\n",
        "# Save the data and the results to a file\n",
        "df.to_csv(data_path+extracted_keys_file, index=False)"
      ],
      "metadata": {
        "id": "5_-uP3x07tUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the output\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "fmXTNgKAgHNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse Entries with Five Keywords"
      ],
      "metadata": {
        "id": "dxmRlXIBYrO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the LLM output to notice that, although in more than half of the cases 5 keywords/keyphrases are extracted, the remaining entries have as little as one keyword or as many as 11 keywords. For the next steps we retain only those titles for which exactly 5 keywords or keyphrases are generated."
      ],
      "metadata": {
        "id": "coOaz9rsa2-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read data from the file - if necessary\n",
        "#df = pd.read_csv(data_path+extracted_keys_file)\n",
        "\n",
        "# The lists are saved as strings in csv, we need to parse them\n",
        "#df['titles_keys'] = df['titles_keys'].apply(ast.literal_eval)\n",
        "\n",
        "# Create a column that records the number of keywords/keyphrases\n",
        "df['keys_length'] = df['titles_keys'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "InQ2T-IBYy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import visualization packages and libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Choose style and color palette\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "colors = sns.color_palette('PuBuGn')\n",
        "\n",
        "# Visualize the keywords length distribution\n",
        "\n",
        "# Adjust figure size and font size\n",
        "sns.set(rc = {'figure.figsize':(6,3)})\n",
        "sns.set(font_scale=.75)\n",
        "\n",
        "# Pad margins so that markers don't get clipped by the axes\n",
        "plt.margins(x=0.2, y=0.2, tight=True);\n",
        "\n",
        "# Plot the keywords length distribution\n",
        "ax = sns.countplot(y='keys_length', data=df, color=colors[4])\n",
        "ax.bar_label(ax.containers[0]);\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Number of keywords/keyphrases per title')\n",
        "plt.xlabel('Keywords')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7rjsGpskhW5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep those titles that have 5 keywords/keyphrases only\n",
        "df5 = df[df.keys_length == 5]\n",
        "\n",
        "# Drop the 'keys_length' columns\n",
        "df5.drop('keys_length', axis=1, inplace=True)\n",
        "\n",
        "# Save the data to a csv file\n",
        "df5.to_csv(data_path+parsed_keys_file, index=False)\n",
        "\n",
        "# Check for success\n",
        "df5.head(10)"
      ],
      "metadata": {
        "id": "Ph4H6NdSpS6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use HDBSCAN & UMAP to Cluster the Keywords"
      ],
      "metadata": {
        "id": "W6-RTuFk7GN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Installs and Imports HDBSCAN & UMAP"
      ],
      "metadata": {
        "id": "GXaOuQK8dP2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this before the next installs to avoid errors\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "pMkpX96GCqjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "!pip install hdbscan\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "DI_OISiRCclz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries and packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Install packages to generate the BERT embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Packages for dimensionality reduction\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Import clustering algorithms\n",
        "import hdbscan\n"
      ],
      "metadata": {
        "id": "BHTektx1Bq0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Keywords for Embedding & Clustering"
      ],
      "metadata": {
        "id": "IYzJiTd7coV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data if needed\n",
        "#df5 = pd.read_csv(data_path+parsed_keys_file)\n",
        "\n",
        "# Create a list of all sublists of keywords and keyphrases\n",
        "df5_keys = df5.titles_keys.tolist()\n",
        "\n",
        "# Flatten the list of sublists\n",
        "flat_keys = [item for sublist in df5_keys for item in sublist]\n",
        "\n",
        "# Create a list of unique keywords\n",
        "flat_keys = list(set(flat_keys))\n",
        "\n",
        "# Create a dataframe with the distinct keywords\n",
        "keys_df = pd.DataFrame(flat_keys, columns = ['key'])\n",
        "\n",
        "# Count how many distinct keywords were extracted\n",
        "keys_df.shape"
      ],
      "metadata": {
        "id": "1XX41U6X8WzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of keywords or keyphrases\n",
        "keys_df.head()"
      ],
      "metadata": {
        "id": "om_peUgaR3-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Embeddings for Clustering"
      ],
      "metadata": {
        "id": "cvHIFLVpddm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Embed the keywords and keyphrases into 768-dim real vector space\n",
        "keys_df['key_bert'] = keys_df['key'].apply(lambda x: model.encode(x))"
      ],
      "metadata": {
        "id": "xqSSHBm9SphS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use UMAP algorithm\n",
        "\n",
        "# Reduce the dimensionality of the vectors to 10 while keeping the size of the local neighborhood to 15\n",
        "embeddings = umap.UMAP(n_neighbors=15,\n",
        "                       n_components=10,\n",
        "                       metric='cosine').fit_transform(list(keys_df.key_bert))\n",
        "\n",
        "# Add the reduced embeddings to the dataframe\n",
        "keys_df['key_umap'] = embeddings.tolist()\n",
        "\n",
        "# Check the output\n",
        "#keys_df.head(2)"
      ],
      "metadata": {
        "id": "PC3ymP4mA7pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster the Keywords with HDBSCAN"
      ],
      "metadata": {
        "id": "yt8uJ8gceJ7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Work with HDBSCAN clustering algorithm\n",
        "\n",
        "# Initialize the clustering model\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(algorithm='best',\n",
        "                            prediction_data=True,\n",
        "                            approx_min_span_tree=True,\n",
        "                            gen_min_span_tree=True,\n",
        "                            min_cluster_size=20,\n",
        "                            cluster_selection_epsilon = .1,\n",
        "                            min_samples=1,\n",
        "                            p=None,\n",
        "                            metric='euclidean',\n",
        "                            cluster_selection_method='leaf')\n",
        "\n",
        "# Fit the data\n",
        "clusterer.fit(embeddings)\n",
        "\n",
        "# Add hard clusters to the data\n",
        "keys_df['hard_labels'] = clusterer.labels_.tolist()\n",
        "\n",
        "# Create soft clusters\n",
        "soft_clusters = hdbscan.all_points_membership_vectors(clusterer)\n",
        "\n",
        "# Add the soft cluster information to the data\n",
        "closest_clusters = [np.argmax(x) for x in soft_clusters]\n",
        "keys_df['cluster'] = closest_clusters\n",
        "\n",
        "# Save embeddings and clusters to a pickle file\n",
        "keys_df.to_pickle(data_path+embeddings_clusters_keys_file)"
      ],
      "metadata": {
        "id": "gMP8VP6yBdxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at one cluster (soft clustering)\n",
        "keys_df[keys_df.cluster==1].key.tolist()"
      ],
      "metadata": {
        "id": "n6oS4edJjJbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clusters Visualizations"
      ],
      "metadata": {
        "id": "3OZRC4-H8xjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import visualization packages and libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Choose style and color palette\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "colors = sns.color_palette('PuBuGn')"
      ],
      "metadata": {
        "id": "uoVrK44DGrQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the keys distribution by hard topics\n",
        "\n",
        "# adjust figure size and font size\n",
        "sns.set(rc = {'figure.figsize':(8,6)})\n",
        "sns.set(font_scale=.60)\n",
        "\n",
        "# pad margins so that markers don't get clipped by the axes\n",
        "plt.margins(x=0.2, y=0.2, tight=True);\n",
        "\n",
        "# plot the page distribution\n",
        "ax = sns.countplot(y='hard_labels', data=keys_df, color=colors[4])\n",
        "ax.bar_label(ax.containers[0]);\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distributions of topics obtained with hard HDBSAN clustering')\n",
        "plt.xlabel('Topic Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gGapqEYROn94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the keys distribution by soft topics\n",
        "\n",
        "# adjust figure size and font size\n",
        "sns.set(rc = {'figure.figsize':(8,8)})\n",
        "sns.set(font_scale=.60)\n",
        "\n",
        "# pad margins so that markers don't get clipped by the axes\n",
        "plt.margins(x=0.2, y=0.2, tight=True);\n",
        "\n",
        "# plot the page distribution\n",
        "ax = sns.countplot(y='cluster', data=keys_df, color=colors[4])\n",
        "ax.bar_label(ax.containers[0]);\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distributions of topics obtained with soft HDBSAN clustering')\n",
        "plt.xlabel('Topic Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jv6Sj_sdIIHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Cluster Descriptions and Labels"
      ],
      "metadata": {
        "id": "wARsJOYdtdT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Clustered Keywords Data"
      ],
      "metadata": {
        "id": "ZkhEY1ntfCBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data if necessary\n",
        "keys_df = pd.read_pickle(data_path+embeddings_clusters_keys_file)\n",
        "\n",
        "# Drop the extra columns\n",
        "keys_df.drop(columns=['key_bert', 'key_umap', 'hard_labels'], inplace=True)\n",
        "\n",
        "# Check the output\n",
        "keys_df.head()"
      ],
      "metadata": {
        "id": "H-qXcCECtcl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Descriptions and Labels for Each Cluster"
      ],
      "metadata": {
        "id": "kV0Fp4JifOhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_description(df: pd.DataFrame,\n",
        "                        n: int\n",
        "                        )-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Use a custom prompt to send to a LLM\n",
        "    to extract labels and descriptions for a list of keywords.\n",
        "    \"\"\"\n",
        "\n",
        "    one_cluster = df[df['cluster']==n]\n",
        "    one_cluster_copy = one_cluster.copy()\n",
        "    sample = one_cluster_copy.key.tolist()\n",
        "\n",
        "    prompt_clusters= f\"\"\"\n",
        "    <|system|>\n",
        "    I have the following list of keywords and keyphrases:\n",
        "    ['encryption','attribute','firewall','security properties',\n",
        "    'network security','reliability','surveillance','distributed risk factors',\n",
        "    'still vulnerable','cryptographic','protocol','signaling','safe',\n",
        "    'adversary','message passing','input-determined guards','secure communication',\n",
        "    'vulnerabilities','value-at-risk','anti-spam','intellectual property rights',\n",
        "    'countermeasures','security implications','privacy','protection',\n",
        "    'mitigation strategies','vulnerability','secure networks','guards']\n",
        "\n",
        "    Based on the information above, first name the domain these keywords or keyphrases belong to, secondly\n",
        "    give a brief description of the domain.\n",
        "    Do not use more than 30 words for the description!\n",
        "    Do not provide details!\n",
        "    Do not give examples of the contexts, do not say 'such as' and do not list the keywords or the keyphrases!\n",
        "    Do not start with a statement of the form 'These keywords belong to the domain of' or with 'The domain'.\n",
        "\n",
        "    Cybersecurity: Cybersecurity, emphasizing methods and strategies for safeguarding digital information\n",
        "    and networks against unauthorized access and threats.\n",
        "    </s>\n",
        "\n",
        "    <|user|>\n",
        "    I have the following list of keywords and keyphrases:\n",
        "    {sample}\n",
        "    Based on the information above, first name the domain these keywords or keyphrases belong to, secondly\n",
        "    give a brief description of the domain.\n",
        "    Do not use more than 30 words for the description!\n",
        "    Do not provide details!\n",
        "    Do not give examples of the contexts, do not say 'such as' and do not list the keywords or the keyphrases!\n",
        "    Do not start with a statement of the form 'These keywords belong to the domain of' or with 'The domain'.\n",
        "    <|assistant|>\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the outputs\n",
        "    outputs = generator(prompt_clusters,\n",
        "                    max_new_tokens=120,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.1,\n",
        "                    top_k=10,\n",
        "                    top_p=0.95)\n",
        "\n",
        "    text = outputs[0][\"generated_text\"]\n",
        "\n",
        "    # Example string\n",
        "    pattern = \"<|assistant|>\\n\"\n",
        "\n",
        "    # Extract the output\n",
        "    response = text.split(pattern, 1)[1].strip(\" \")\n",
        "\n",
        "    if len(response.split(\":\", 1)) == 2:\n",
        "        label  = response.split(\":\", 1)[0].strip(\" \")\n",
        "        description = response.split(\":\", 1)[1].strip(\" \")\n",
        "    else:\n",
        "        label = description = response\n",
        "\n",
        "    # Add the description and the labels to the dataframe\n",
        "    one_cluster_copy.loc[:, 'description'] = description\n",
        "    one_cluster_copy.loc[:, 'label'] = label\n",
        "\n",
        "    return one_cluster_copy"
      ],
      "metadata": {
        "id": "9CuI0W1apnz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty list to store the cluster dataframes\n",
        "dataframes = []\n",
        "clusters = len(set(keys_df.cluster))\n",
        "\n",
        "# Iterate over the range of n values\n",
        "for n in range(clusters-1):\n",
        "    df_result = extract_description(keys_df,n)\n",
        "    dataframes.append(df_result)\n",
        "\n",
        "# Concatenate all the dataframes\n",
        "final_df = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "qmPyO5rP_UD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check if there are any faulty otputs\n",
        "faulty_df = final_df[final_df['description'] == final_df['label']]\n",
        "set(faulty_df.cluster.tolist())"
      ],
      "metadata": {
        "id": "bvIbClxviuj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate the output - also need to check manually\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "final_df.drop_duplicates(subset='cluster')"
      ],
      "metadata": {
        "id": "OWIJGFxmkUc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comments:**  \n",
        "There are a few changes we can make to clean up the output:\n",
        "- rephrase the labels and descriptions for the faulty clusters;\n",
        "- check if there are any repeated labels and change accordingly."
      ],
      "metadata": {
        "id": "ma8SSkKqlV8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the information for label 7\n",
        "final_df.loc[final_df['cluster'] == 7, 'label'] = 'Constraint-Based Natural Language Processing'\n",
        "final_df.loc[final_df['cluster']== 7, 'description'] = 'This is an area of research that focuses on using constraints to improve the accuracy and efficiency of natural language processing tasks such as parsing, semantic role labeling, and question answering.'\n"
      ],
      "metadata": {
        "id": "61-_TPuJk5PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check label 14 entries\n",
        "cluster_14 = keys_df[keys_df['cluster']==14]\n",
        "keys_14 = cluster_14.key.tolist()"
      ],
      "metadata": {
        "id": "AUXupNxKrYFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the information for label 14\n",
        "final_df.loc[final_df['cluster'] == 14, 'label'] = 'Automated Reasoning'\n",
        "final_df.loc[final_df['cluster']== 14, 'description'] = 'Research focused on advancing techniques for automated reasoning and verification, particularly in the areas of bisimulations, enrichments, and selective interleaving functions, with an emphasis on improving and extending existing algorithms and results.'"
      ],
      "metadata": {
        "id": "AM-oLydqqAWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the information for label 29\n",
        "final_df.loc[final_df['cluster'] == 29, 'label'] = 'Vulnerabilities in Secure Networks'\n",
        "final_df.loc[final_df['cluster']== 29, 'description'] = 'Refers to mitigation strategies for vulnerabilities in secure networks'"
      ],
      "metadata": {
        "id": "wq-V98FP1Lew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate the output\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "final_df.drop_duplicates(subset='cluster')"
      ],
      "metadata": {
        "id": "KpQQNemAm-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for repeated labels\n",
        "len(set(final_df.cluster.tolist())) , len(set(final_df.label.tolist()))"
      ],
      "metadata": {
        "id": "bABiE5B5nSFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the repeated labels\n",
        "\n",
        "test = final_df.drop_duplicates(subset='cluster')\n",
        "\n",
        "# Find duplicates in 'label' column and get corresponding 'cluster' values\n",
        "duplicate_labels_with_clusters = test[test['label'].duplicated(keep=False)]\n",
        "\n",
        "print(\"All Duplicates with Clusters:\\n\")\n",
        "duplicate_labels_with_clusters.sort_values(by=['label'])"
      ],
      "metadata": {
        "id": "wmP9yw6Z2LAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the repeated labels\n",
        "final_df.loc[final_df['cluster'] == 53, 'label'] = 'Data Protection'\n",
        "final_df.loc[final_df['cluster'] == 16, 'label'] = 'Computational Graph Theory'\n",
        "final_df.loc[final_df['cluster'] == 10, 'label'] = 'Efficient Information Retrieval'\n",
        "final_df.loc[final_df['cluster'] == 11, 'label'] = 'Set Theory'\n",
        "final_df.loc[final_df['cluster'] == 24, 'label'] = 'Equivalence Relations'\n",
        "final_df.loc[final_df['cluster'] == 50, 'label'] = 'Software Development Principles'"
      ],
      "metadata": {
        "id": "lREutYTptsEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for success\n",
        "len(set(final_df.cluster.tolist())) , len(set(final_df.label.tolist()))"
      ],
      "metadata": {
        "id": "He_iTvVQvz64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to a file\n",
        "final_df.to_csv(data_path+descriptions_labels_keys_file, index=False)"
      ],
      "metadata": {
        "id": "PjGuJJD8v0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Knowledge Graph"
      ],
      "metadata": {
        "id": "Fd-2cHHWgjm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Neo4j Dependencies"
      ],
      "metadata": {
        "id": "RAqsNDdPDEYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "id": "KCzMoiTPDInV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Structure Data"
      ],
      "metadata": {
        "id": "7q0PLDr-WN2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Articles, titles, abstracts and keywords\n",
        "articles = pd.read_csv(data_path+parsed_keys_file)\n",
        "# Keywords, soft labels/clusters, descriptions and labels\n",
        "keywords = pd.read_csv(data_path+descriptions_labels_keys_file)"
      ],
      "metadata": {
        "id": "-ALA4fr_hzig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "# The keywords column entries must be a list\n",
        "articles['titles_keys'] = articles['titles_keys'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "chP2vBaJu1Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the articles dataframe\n",
        "articles.head(2)"
      ],
      "metadata": {
        "id": "nTmQMsAprfVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the keywords dataframe\n",
        "keywords.head(2)"
      ],
      "metadata": {
        "id": "dgB8Z-x2re8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Establish Neo4j Connection"
      ],
      "metadata": {
        "id": "-W50wb0eFNj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Neo4j connector module\n",
        "from utils.neo4j_conn import *"
      ],
      "metadata": {
        "id": "LyOz86CDBaCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Neo4j AuraDB free instance and collect the credentials\n",
        "URI = 'neo4j+ssc://xxxxxxxx.databases.neo4j.io'\n",
        "USER = 'neo4j'\n",
        "PWD = 'your_password'\n",
        "\n",
        "# Initialize the Neo4j connector\n",
        "graph=Neo4jGraph(url=URI, username=USER, password=PWD)"
      ],
      "metadata": {
        "id": "kLFvWZ_jp66p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the connection for success\n",
        "graph.query(\"MATCH (n) RETURN count(n)\")"
      ],
      "metadata": {
        "id": "tmPO_nI3xA0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%md\n",
        "### Build the Knowledge Graph"
      ],
      "metadata": {
        "id": "LpiXF0ehB4ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create uniqueness constraints on each node\n",
        "constraints_articles = \"\"\"\n",
        "CREATE CONSTRAINT article_id IF NOT EXISTS FOR (article:Article) REQUIRE article.id IS UNIQUE;\"\"\"\n",
        "graph.query(constraints_articles)\n",
        "constraints_keywords=\"\"\"\n",
        "CREATE CONSTRAINT keyword_key IF NOT EXISTS FOR (keyword:Keyword) REQUIRE keyword.key IS UNIQUE;\n",
        "\"\"\"\n",
        "graph.query(constraints_keywords)\n",
        "constraints_clusters=\"\"\"\n",
        "CREATE CONSTRAINT topic_cluster IF NOT EXISTS FOR (topic:Topic) REQUIRE topic.cluster IS UNIQUE;\n",
        "\"\"\"\n",
        "graph.query(constraints_clusters)"
      ],
      "metadata": {
        "id": "TUhsvZlqjqMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Keyword and Topic nodes, and the relationship HAS_TOPIC\n",
        "query_keywords_topics = \"\"\"\n",
        "    UNWIND $rows AS row\n",
        "    MERGE (k:Keyword {name: row.key})\n",
        "    MERGE (t:Topic {cluster: row.cluster, description: row.description, label: row.label})\n",
        "    MERGE (k)-[:HAS_TOPIC]->(t)\n",
        "    \"\"\"\n",
        "graph.load_data(query_keywords_topics, keywords)"
      ],
      "metadata": {
        "id": "6OUgnm_eqEzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Article nodes and the relationship HAS_KEY\n",
        "query_articles = \"\"\"\n",
        "    UNWIND $rows as row\n",
        "    MERGE (a:Article {id: row.id, title: row.title, abstract: row.abstract})\n",
        "    WITH a, row\n",
        "    UNWIND row.titles_keys as key\n",
        "    MATCH (k:Keyword {name: key})\n",
        "    MERGE (a)-[:HAS_KEY]->(k)\n",
        "    \"\"\"\n",
        "graph.load_data(query_articles, articles)"
      ],
      "metadata": {
        "id": "2kYdTFaos7on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query the Knowledge Graph"
      ],
      "metadata": {
        "id": "sYevn9wQkwUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the nodes in the graph, grouped by node labels\n",
        "\n",
        "cypher_statement = \"\"\"\n",
        "MATCH (n)\n",
        "RETURN HEAD(LABELS(n)) AS Node, COUNT(n) AS Total\n",
        "ORDER BY Total DESC\n",
        "\"\"\"\n",
        "\n",
        "result = graph.query(cypher_statement)\n",
        "df = pd.DataFrame(result)\n",
        "df"
      ],
      "metadata": {
        "id": "uoAcvApkpkl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the relationships in the graph, grouped by type\n",
        "\n",
        "cypher_statement = \"\"\"\n",
        "MATCH ()-[r]-()\n",
        "RETURN TYPE(r) AS Relationship, COUNT(r) AS Total\n",
        "ORDER BY Total DESC\n",
        "\"\"\"\n",
        "\n",
        "result = graph.query(cypher_statement)\n",
        "df = pd.DataFrame(result)\n",
        "df"
      ],
      "metadata": {
        "id": "nk2PQ9BWp93u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the counts of articles associated with each topic through the keywords\n",
        "\n",
        "cypher_statement = \"\"\"\n",
        "MATCH (a:Article)-[:HAS_KEY]->(k:Keyword)-[:HAS_TOPIC]->(t:Topic)\n",
        "RETURN t.label AS Label, t.cluster AS Cluster, COUNT(a) AS Articles\n",
        "ORDER BY Articles DESC\n",
        "\"\"\"\n",
        "\n",
        "result = graph.query(cypher_statement)\n",
        "df = pd.DataFrame(result)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2ey8y1M2i3QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the five most common keywords shared by articles within the same topic?\n",
        "\n",
        "cypher_statement = \"\"\"\n",
        "MATCH (t:Topic)<-[:HAS_TOPIC]-(:Keyword)<-[:HAS_KEY]-(a:Article)\n",
        "WITH t, a\n",
        "MATCH (a)-[:HAS_KEY]->(k:Keyword)\n",
        "RETURN t.label AS Label, k.name AS Keyword, COUNT(*) AS SharedCount\n",
        "ORDER BY SharedCount DESC\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "result = graph.query(cypher_statement)\n",
        "df = pd.DataFrame(result)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h1YRdRyei3NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find pairs of articles that are indirectly connected through a chain of length up to 4\n",
        "\n",
        "cypher_statement = \"\"\"\n",
        "MATCH path = (a1:Article)-[:HAS_KEY|HAS_TOPIC*2..4]-(a2:Article)\n",
        "WHERE a1 <> a2\n",
        "RETURN a1.title AS Title1, a2.title AS Title2, LENGTH(path) AS PathLength\n",
        "ORDER BY PathLength DESC\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "result = graph.query(cypher_statement)\n",
        "df = pd.DataFrame(result)\n",
        "df"
      ],
      "metadata": {
        "id": "_RVzCWJhi3Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYoUSqNX0BBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}