{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Description"
      ],
      "metadata": {
        "id": "I-YWUGyBBHCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook:\n",
        "\n",
        "- upload TheBloke/Mixtral_7Bx2_MoE-GPTQ from HuggningFace,\n",
        "- use the model to generate categories and sample question:cypher pairs.\n",
        "\n",
        "These are tests only! Also notice this is a 7Bx2 and not 7Bx8 MoE."
      ],
      "metadata": {
        "id": "8Y-DpFNiBMc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workspace Setup"
      ],
      "metadata": {
        "id": "ZXaqcLlZaMB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide HuggingFace token - step not required\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "fIaKu2BriuAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b8WkPzR140F"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade transformers optimum\n",
        "!pip3 install --upgrade auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and mount the drive helper\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory\n",
        "%cd '/content/drive/MyDrive/cypherGen/'\n",
        "\n",
        "# Create a path variable for the data folder\n",
        "data_path = '/content/drive/MyDrive/cypherGen/datas/'"
      ],
      "metadata": {
        "id": "dxV5ZLdJaMqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "\n",
        "# Import the local modules\n",
        "from utils.utilities import *"
      ],
      "metadata": {
        "id": "h0ma9vdfaMTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mixtral_7Bx2_MoE-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "Gn5nao1whfDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference can be done using transformers' pipeline\n",
        "\n",
        "def gen_text(prompt_template):\n",
        "    print(\"*** Text Generation Pipeline:\\n\")\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.5, #0.7\n",
        "        top_p=0.95,\n",
        "        top_k=10, # 40 on HF, 50 default\n",
        "        repetition_penalty=1.1\n",
        "        )\n",
        "    return pipe(prompt_template) #print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "iiXd4NzA-5k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Categories of Queries"
      ],
      "metadata": {
        "id": "whxWHUNA8XLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt has to be adjusted to the LLM by using the specific meta-tokens\n",
        "prompt_cat = \"\"\"I have a knowledge graph for which I would like to generate\n",
        "about 1000 very interesting questions which span 10 categories (or types) about the graph.\n",
        "They should cover single nodes questions, two or three more nodes, many relationships questions.\n",
        " Please provide these 10 categories.\n",
        "\n",
        " Here is the graph schema:\n",
        "Node properties are the following:\\n\n",
        "Article {abstract: STRING, article_id: INTEGER, comments: STRING, title: STRING},\n",
        "Keyword {name: STRING, key_id: STRING},\n",
        "Topic {cluster: INTEGER, description: STRING, label: STRING},\n",
        "Author {author_id: STRING, affiliation: STRING,first_name: STRING, last_name: STRING},\n",
        "DOI {name: STRING, doi_id: STRING},\n",
        "Categories {category_id: STRING, specifications: STRING},\n",
        "Report {report_id: STRING, report_no: STRING},\n",
        "UpdateDate {update_date: DATE},\n",
        "Journal {name: STRING, journal_id: STRING}\\n\n",
        "Relationship properties are the following:\\n\n",
        "PUBLISHED_IN {meta: STRING, pages: STRING, year: INTEGER}\\n\n",
        "The relationships are the following:\\n\n",
        "(:Article)-[:HAS_KEY]->(:Keyword),\n",
        "(:Article)-[:HAS_DOI]->(:DOI),\n",
        "(:Article)-[:HAS_CATEGORY]->(:Categories),\n",
        "(:Article)-[:WRITTEN_BY]->(:Author),\n",
        "(:Article)-[:UPDATED]->(:UpdateDate),\n",
        "(:Article)-[:PUBLISHED_IN]->(:Journal),\n",
        "(:Article)-[:HAS_REPORT]->(:Report),\n",
        "(:Keyword)-[:HAS_TOPIC]->(:Topic)\"\n",
        "\"\"\"\n",
        "system_message = \"You are an experienced, very helpful Python and Neo4j/Cypher developer.\"\n",
        "prompt_template_cat=f'''<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt_cat}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "print(\"\\n\\n*** Generate:\")"
      ],
      "metadata": {
        "id": "oe5Niv7nh4iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate categories using text generation pipeline\n",
        "test = gen_text(prompt_template_cat)"
      ],
      "metadata": {
        "id": "w7pBz1WoAF21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vDSdU6JYAVLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate categories using model generate\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids,\n",
        "                        temperature=0.7,\n",
        "                        do_sample=True,\n",
        "                        top_p=0.95,\n",
        "                        top_k=40,\n",
        "                        max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "LMEeFu0Jh9R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Individual Pairs Question:Cypher"
      ],
      "metadata": {
        "id": "WQkOJrCS8gue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_pair = \"\"\"Generate 10 questions and their corresponding Cypher statements\n",
        "about the Neo4j graph database with the following schema:\n",
        "Node properties are the following:\\n\n",
        "Article {abstract: STRING, article_id: INTEGER, comments: STRING, title: STRING},\n",
        "Keyword {name: STRING, key_id: STRING},\n",
        "Topic {cluster: INTEGER, description: STRING, label: STRING},\n",
        "Author {author_id: STRING, affiliation: STRING,first_name: STRING, last_name: STRING},\n",
        "DOI {name: STRING, doi_id: STRING},\n",
        "Categories {category_id: STRING, specifications: STRING},\n",
        "Report {report_id: STRING,report_no: STRING},\n",
        "UpdateDate {update_date: DATE},\n",
        "Journal {name: STRING, journal_id: STRING}\\n\n",
        "Relationship properties are the following:\\n\n",
        "PUBLISHED_IN {meta: STRING, pages: STRING, year: INTEGER}\\n\n",
        "The relationships are the following:\\n\n",
        "(:Article)-[:HAS_KEY]->(:Keyword),\n",
        "(:Article)-[:HAS_DOI]->(:DOI),\n",
        "(:Article)-[:HAS_CATEGORY]->(:Categories),\n",
        "(:Article)-[:WRITTEN_BY]->(:Author),\n",
        "(:Article)-[:UPDATED]->(:UpdateDate),\n",
        "(:Article)-[:PUBLISHED_IN]->(:Journal),\n",
        "(:Article)-[:HAS_REPORT]->(:Report),\n",
        "(:Keyword)-[:HAS_TOPIC]->(:Topic)\n",
        "The questions should be article based and should be phrased in a natural conversational manner.\n",
        "Make the questions diverse and interesting.\n",
        "Make sure to use the latest Cypher version and that all the queries are working Cypher queries for the provided graph.\n",
        "You may add values for the node attributes as needed.\n",
        "Do not add any comments, do not label or number the questions.\"\"\"\n",
        "system_message = \"You are an experienced and useful Python and Neo4j/Cypher developer. \"\n",
        "prompt_template=f'''<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt_pair}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "print(\"\\n\\n*** Generate:\")"
      ],
      "metadata": {
        "id": "4Ft8KID-pEOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids,\n",
        "                        temperature=0.1,\n",
        "                        do_sample=True,\n",
        "                        top_p=0.95,\n",
        "                        top_k=10,\n",
        "                        max_new_tokens=512) # increase to at least 1K\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "ED59XPIXAMS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZTkxVMzAMMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}